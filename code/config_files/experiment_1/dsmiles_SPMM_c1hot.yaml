dataset_dir: "/home/grads/tasnina/Projects/SynVerse/datasets/"
# Input Settings: initialize base input folder names,
# dataset collections, and models to run over
input_settings:
  # Base input directory
  input_dir: "/home/grads/tasnina/Projects/SynVerse/inputs/"
  drug_features:
    - name: 'MACCS'
      use: [ false ]

    - name: 'ECFP_4'
      use: [ false ]

    - name: 'MFP' #morgan fingerprint
      use: [ false ]

    - name: 'mol_graph'
      encoder: 'GCN'
      use: [ false ]

    - name: 'smiles'
      encoder: 'SPMM'
      use: [ true ]


    - name: 'd1hot'
      use: [ false ]

  cell_line_features:
    - name: 'c1hot'
      use: [true]

  models:
     decoder:
       name: 'MLP'
       hp_range: { 'lr': [ 1e-5, 1e-4 ], 'optimizer': [ 'Adam', 'SGD' ] , 'sgd_momentum': [ 0.0, 0.99 ] ,
                   'num_hid_layers': [ 1,3 ] , 'hid_0': [ 64, 2048 ] , 'hid_1': [ 64,2048 ] , 'hid_2': [ 64, 2048 ] ,
                   'in_dropout_rate': [ 0.0, 0.5 ] ,'hid_dropout_rate': [ 0.0, 0.5 ] }
       hp: { 'hid_0': 107, 'hid_dropout_rate': 0.1979690297048191, 'in_dropout_rate': 0.3484321779065158,
            'lr': 0.00035603226284629366, 'num_hid_layers': 2, 'optimizer': 'Adam', 'hid_1': 1276 }

     drug_encoder:
       - name: 'GCN'
         hp_range: { 'batch_norm': [ True, False ], 'gnn_num_layers': [ 1,3 ], 'gnn_0': [ 64, 2048 ] ,'gnn_1': [ 64, 2048 ],'gnn_2': [ 64, 2048 ] ,
                     'ff_num_layers': [ 1,3 ], 'ff_0': [ 16, 2048 ] , 'ff_1': [ 16, 2048 ] , 'ff_2': [ 16, 2048 ] , 'gnn_dropout': [ 0.0,0.5 ] }
         hp: { 'batch_norm': True, 'gnn_num_layers': 2 ,'gnn_0': 512 ,'gnn_1': 128,
               'ff_num_layers': 1, 'ff_0': 128, 'gnn_dropout': 0.3 }

       - name: 'Transformer'
         hp_range: { 'transformer_batch_norm': [ True, False ], 'transformer_num_layers': [2, 3, 4] ,'transformer_embedding_dim': [32, 64, 128, 256, 512] ,
             'transformer_n_head': [4, 8], 'transformer_ff_num_layers': [32, 64, 128, 256, 512, 1024], 'max_seq_length': [120],
              'positional_encoding_type': ['learnable', 'fixed'] }
         hp: { 'transformer_batch_norm': True, 'transformer_num_layers': 3 ,'transformer_embedding_dim': 128 , #512 worked
             'transformer_n_head': 4, 'transformer_ff_num_layers': 64, 'max_seq_length': 120, 'positional_encoding_type': 'fixed' }

       - name: 'SPMM'
         params:
           {
              checkpoint: '/home/grads/tasnina/Projects/SynVerse/inputs/drug/pretrain/checkpoint_SPMM.ckpt',
              vocab: '/home/grads/tasnina/Projects/SynVerse/inputs/drug/vocab_bpe_300.txt'
           }

     cell_encoder: null

  batch_size: 2048
  epochs: 1000


  splits:
    - type: 'leave_drug'
      test_frac: 0.2
      n_folds: 5

    - type: 'leave_comb'
      test_frac: 0.2
      n_folds: 5

    - type: 'leave_cell_line'
      test_frac: 0.2
      n_folds: 5

  wandb:
    enabled: True
    token: 'd9462b91edea6523563900fab17134d7e9177e16'
    project_name: 'Synverse'
    entity_name: 'ntasnina'
    timezone: 'US/Eastern'
    timezone_format: '%Y-%m-%d_%H-%M-%S'
  bohb:
    min_budget: 9
    max_budget: 243
    n_iterations: 3
    run_id : 'synverse_smiles'
    server_type: 'local' #or cluster

  feature: 'must'
  abundance: 0.05

  max_feat: 1
  mode: 'debug'
        #'hp_tune' = hyperparam tuning and training the model with both train+val dataset
        # 'train' =  training the model with both train+val dataset
        # 'train_val' = train the model on training data and validate it on validation fold
        # 'debug' = train the model on training data and validate it on validation fold. Then  train the model with both train+val dataset and return test loss.


output_settings:
  output_dir: '/home/grads/tasnina/Projects/SynVerse/outputs/'